{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import docx\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len, is_separator_regex=False)\n",
    "# Define functions for processing\n",
    "# def get_docx_files_dict(folder_path):\n",
    "#     \"\"\"Get a dictionary of .docx files with their 3GPP spec numbers.\"\"\"\n",
    "#     result = {}\n",
    "#     count = 0\n",
    "#     for root, dirs, files in os.walk(folder_path):\n",
    "#         for file in files:\n",
    "#             if file.endswith('.docx'):\n",
    "#                 file_path = os.path.join(root, file)\n",
    "#                 match = re.search(r'(\\d{5})-', file)\n",
    "#                 if match:\n",
    "#                     spec_number = match.group(1)\n",
    "#                     series = spec_number[:2]\n",
    "#                     spec_number = spec_number[2:]\n",
    "#                     key = f\"3GPP TS {series}.{spec_number}\"\n",
    "#                     result[count] = [key, file_path]\n",
    "#                     count += 1\n",
    "#     return result\n",
    "\n",
    "\n",
    "\n",
    "def get_docx_files_dict(folder_path):\n",
    "    \"\"\"Get a dictionary of .docx files with their 3GPP spec numbers.\"\"\"\n",
    "    result = {}\n",
    "    count = 0\n",
    "    \n",
    "    # Check if the folder path exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        raise ValueError(f\"The specified folder path does not exist: {folder_path}\")\n",
    "    \n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.docx'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                match = re.search(r'(\\d{5})-', file)\n",
    "                \n",
    "                if match:\n",
    "                    spec_number = match.group(1)\n",
    "                    series = spec_number[:2]\n",
    "                    spec_number = spec_number[2:]\n",
    "                    key = f\"3GPP TS {series}.{spec_number}\"\n",
    "                    result[count] = [key, file_path]\n",
    "                    count += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "def extract_abb(docpath, pattern):\n",
    "    \"\"\"Extract abbreviations from the provided docx file.\"\"\"\n",
    "    doc = docx.Document(docpath)\n",
    "    abbreviation_section_started = False\n",
    "    abbreviations = []\n",
    "    count = 0\n",
    "    max_lines = 200\n",
    "    cnt = 0\n",
    "    for paragraph in doc.paragraphs:\n",
    "        if 'Abbreviations' in paragraph.text:\n",
    "            abbreviation_section_started = True\n",
    "            cnt += 1\n",
    "            continue\n",
    "        if cnt < 2:\n",
    "            continue\n",
    "        if abbreviation_section_started and count < max_lines:\n",
    "            if paragraph.text.strip():\n",
    "                abbreviations.append(paragraph.text.strip())\n",
    "            count += 1\n",
    "        if count >= max_lines:\n",
    "            break\n",
    "    abbreviation_dict = {}\n",
    "    for line in abbreviations:\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) != 2:\n",
    "            continue\n",
    "        if re.match(pattern, parts[0]):\n",
    "            abbreviation_dict[parts[0]] = parts[1]\n",
    "    return abbreviation_dict\n",
    "\n",
    "def extract_sections_from_docx(docx_file):\n",
    "    \"\"\"Extract sections from a .docx file.\"\"\"\n",
    "    try:\n",
    "        doc = docx.Document(docx_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening {docx_file}: {e}\")\n",
    "        return {}\n",
    "    sections = {}\n",
    "    current_title = None\n",
    "    current_content = []\n",
    "    for para in doc.paragraphs:\n",
    "        if para.style.name.startswith('Heading'):\n",
    "            if current_title:\n",
    "                sections[current_title] = \"\\n\".join(current_content)\n",
    "            current_title = para.text.strip()\n",
    "            current_content = []\n",
    "        else:\n",
    "            if para.text.strip():\n",
    "                current_content.append(para.text.strip())\n",
    "    if current_title:\n",
    "        sections[current_title] = \"\\n\".join(current_content)\n",
    "    return sections\n",
    "\n",
    "def get_context_metadata(folder_path, pattern, skip_keywords, text_splitter):\n",
    "    \"\"\"Create a metadata mapping from abbreviations to sections.\"\"\"\n",
    "    docs = get_docx_files_dict(folder_path)\n",
    "    context_to_metadata = defaultdict(list)\n",
    "    for key, val in tqdm(docs.items()):\n",
    "        doc_number, doc_path = val[0], val[1]\n",
    "        sections = extract_sections_from_docx(doc_path)\n",
    "        main_title = \"\"\n",
    "        for section_title, content in sections.items():\n",
    "            title = re.sub(r'\\s+', ' ', section_title.strip())\n",
    "            content = re.sub(r'\\s+', ' ', content.strip())\n",
    "            if any(keyword.lower() in title.lower() for keyword in skip_keywords):\n",
    "                if 'Abbreviations'.lower() in title.lower():\n",
    "                    continue\n",
    "            if content == \"\":\n",
    "                main_title = title\n",
    "                continue\n",
    "            if \"general\" in title.lower():\n",
    "                title = f\"{main_title} {title}\"\n",
    "            texts = text_splitter.create_documents([content])\n",
    "            chunks = [chunk.page_content for chunk in texts]\n",
    "            for text in chunks:\n",
    "                context_to_metadata[text] = [section_title, doc_number]\n",
    "        print(len(context_to_metadata))\n",
    "    return context_to_metadata\n",
    "\n",
    "def get_exact_count(abbreviation, list_content):\n",
    "    \"\"\"Get the exact frequency of an abbreviation in the content.\"\"\"\n",
    "    freq_count = 0\n",
    "    best_context = \"\"\n",
    "    for content in list_content:\n",
    "        count = content.lower().split().count(abbreviation.lower())\n",
    "        if count > freq_count:\n",
    "            freq_count = count\n",
    "            best_context = content\n",
    "    print(f\"Best context for '{abbreviation}' (appeared {freq_count} times): {best_context[:100]}\") \n",
    "    return best_context, freq_count\n",
    "\n",
    "\n",
    "def process_abbreviations(folder_path, pattern):\n",
    "    \"\"\"Extract abbreviations from all docx files and build the abbreviation list.\"\"\"\n",
    "    all_abbreviations = []\n",
    "    processed_abbreviation_set = set()\n",
    "    docx_files = get_docx_files_dict(folder_path)\n",
    "    for _, (key, file_path) in tqdm(docx_files.items()):\n",
    "        processed_abbreviations = extract_abb(file_path, pattern)\n",
    "        print(f\"Extracted abbreviations from {file_path}: {processed_abbreviations}\")\n",
    "        for abb, full_form in processed_abbreviations.items():\n",
    "            if abb not in processed_abbreviation_set:\n",
    "                processed_abbreviation_set.add(abb)\n",
    "                all_abbreviations.append({\"name\": abb, \"full_form\": full_form})\n",
    "    print(f\"Total unique abbreviations extracted: {len(all_abbreviations)}\")\n",
    "    return all_abbreviations\n",
    "\n",
    "def collected_data(output_path, all_abbreviations, meta_data):\n",
    "    \"\"\"Save the processed abbreviations to a CSV file with relevant metadata.\"\"\"\n",
    "    list_context = list(meta_data.keys())\n",
    "    univ = []\n",
    "    for entry in all_abbreviations:\n",
    "        name = entry['name']\n",
    "        full_form = entry['full_form']\n",
    "        rich_context, freq = get_exact_count(name, list_context)\n",
    "        print(rich_context)\n",
    "        if freq <= 1:\n",
    "            continue\n",
    "        metadata_list = meta_data.get(rich_context, [])\n",
    "        if len(metadata_list) < 1:\n",
    "            print(f\"Skipping entry for {name}: Incomplete metadata for context.\")\n",
    "            continue\n",
    "        rec = {\n",
    "            \"name\": name,\n",
    "            \"full_form\": full_form,\n",
    "            \"context\": rich_context,\n",
    "            \"section_title\": metadata_list[0],\n",
    "            \"doc_number\": metadata_list[1]\n",
    "        }\n",
    "        print(rec)\n",
    "        univ.append(rec)\n",
    "    if univ:\n",
    "        data = pd.DataFrame(univ)\n",
    "        data.to_csv(output_path, index=False)\n",
    "        print(f\"Successfully saved {len(univ)} entries to output file: {output_path}\")\n",
    "    else:\n",
    "        print(\"No valid data to save.\")\n",
    "\n",
    "# Example usage\n",
    "folder_path = '/home/sunny/Desktop/SunnyKG/specs_22_31_33'\n",
    "pattern = r'\\b(?!NOTE\\b)(?!NOTE\\d)(?!\\d+$)(?!.*[.)])(?![A-Z0-9]{1}\\b)(?!\\d+[>])(?<![^\\s])(?<![\\d])(?<!\\d\\s)[A-Z0-9]+(?:-[A-Z0-9]+)*\\b'\n",
    "skip_keywords = [\n",
    "    \"Foreword\", \n",
    "    \"Scope\", \n",
    "    \"References\", \n",
    "    \"Definitions\", \n",
    "    \"Abbreviations\", \n",
    "    \"Definitions and abbreviations\", \n",
    "    \"Definitions, symbols and abbreviations\"\n",
    "]\n",
    "\n",
    "meta_data = get_context_metadata(folder_path, pattern, skip_keywords, text_splitter)\n",
    "\n",
    "all_abbreviations = process_abbreviations(folder_path, pattern)\n",
    "output_path = \"Ran_collected_data_22_31_33.csv\"\n",
    "collected_data(output_path, all_abbreviations, meta_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# above code for automation data collection preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## below code for mapping to working group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speci=pd.read_csv(\"/home/sunny/Desktop/SunnyKG/prev_method/specifications.csv\")\n",
    "speci_wkg={}\n",
    "for _,row in speci.iterrows():\n",
    "    wkg=row['name']\n",
    "    doc_title=row['spec_detail']\n",
    "    doc_number=row['spec_number']\n",
    "    speci_wkg[doc_number]=[wkg,doc_title]\n",
    "import re\n",
    "pattern = r'\\bTS \\d{2}\\.\\d{3}\\b'\n",
    "def extract_ts_pattern(text):\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.group(0)  \n",
    "    return None\n",
    "data=pd.read_csv(\"/home/sunny/Desktop/SunnyKG/prev_method/Ran_collected_data_32_38_34.csv\")\n",
    "\n",
    "data['doc_title'] = (\n",
    "    data['doc_number']\n",
    "    .apply(lambda x: extract_ts_pattern(x))\n",
    "    .apply(lambda x: speci_wkg.get(x, [None, None])[1])  \n",
    ")\n",
    "\n",
    "data['working_Group'] = (\n",
    "    data['doc_number']\n",
    "    .apply(lambda x: extract_ts_pattern(x))\n",
    "    .apply(lambda x: speci_wkg.get(x, [None, None])[0]) \n",
    ")\n",
    "\n",
    "# Display the DataFrame to verify\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"Ran_data_wkg.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##context evaluataion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Message Content: {  \n",
      "  \"Selected word\": \"ACLR\"\n",
      "}\n",
      "AI Message Content: {  \n",
      "  \"Selected word\": \"ACS\"\n",
      "}\n",
      "AI Message Content: {  \n",
      "  \"Selected word\": \"AWGN\"\n",
      "}\n",
      "AI Message Content: {\n",
      "  \"Selected word\": \"BW\"\n",
      "}\n",
      "AI Message Content: {  \n",
      "  \"Selected word\": \"CW\"\n",
      "}\n",
      "AI Message Content: {  \n",
      "  \"Selected word\": \"DL\"\n",
      "}\n",
      "AI Message Content: {\n",
      "  \"Selected word\": \n",
      "    -1.2 \n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    }\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 81\u001b[0m\n\u001b[1;32m     76\u001b[0m     options \u001b[38;5;241m=\u001b[39m get_random_options(vocab, name)\n\u001b[1;32m     77\u001b[0m     input_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasked_context\u001b[39m\u001b[38;5;124m\"\u001b[39m:masked_context,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m:options\n\u001b[1;32m     78\u001b[0m         \n\u001b[1;32m     79\u001b[0m     })\n\u001b[0;32m---> 81\u001b[0m     corrected\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m(\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput_string\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSelected word\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m())\u001b[38;5;241m==\u001b[39mname\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m given this much accuracy for context\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m accuracy[model]\u001b[38;5;241m=\u001b[39mcorrected\u001b[38;5;241m/\u001b[39mtotal\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "system_prompt = \"\"\"\n",
    "You are a context evaluator specializing in telecommunications specifications. Your task is to interpret the provided context and accurately fill in the missing term marked as \"<fill_here>\" by selecting the most appropriate word from four given options. Your response should provide only the selected term, which best completes the sentence technically and contextually.\n",
    "\n",
    "### Instructions:\n",
    "1. **Contextual Understanding**: Analyze the context sentence carefully to understand the meaning and technical requirements of the missing term.\n",
    "2. **Selection Criteria**:\n",
    "   - **Technical Relevance**: The word chosen should be accurate in the 3GPP telecommunications domain.\n",
    "   - **Contextual Fit**: Select the option that makes the most sense within the sentence provided, ensuring coherence and relevance.\n",
    "3. **Strict Output Requirement**: \n",
    "   - Your output must strictly follow this format: **\"Selected word: <Chosen Option>\"**.\n",
    "   - Do not include additional information, explanations, symbols, numbers, or line breaks.\n",
    "   - Only return one word from the provided options, exactly as instructed.\n",
    "\n",
    "### Input Format:\n",
    "{\n",
    "  \"context\": \"Sentence containing <fill_here> placeholders\",\n",
    "  \"options\": [\"Option1\", \"Option2\", \"Option3\", \"Option4\"]\n",
    "}\n",
    "\n",
    "### Output Format:\n",
    "Selected word: <Chosen Option>\n",
    "\n",
    "### Example Input:\n",
    "{\n",
    "  \"context\": \"The primary role of the <fill_here> function is to manage network connectivity for user equipment.\",\n",
    "  \"options\": [\"AMF\", \"RRC\", \"IMSI\", \"QoS\"]\n",
    "}\n",
    "\n",
    "### Expected Output:\n",
    "Selected word: AMF\n",
    "\n",
    "### Note:\n",
    "You are a **context evaluator**. Choose only one word from the provided options that best completes the \"<fill_here>\" term in the context sentence.\n",
    "\n",
    "\"\"\"\n",
    "models=[\"nemotron:latest\",\"llama3.1:70b\",\"gemma2:9b\"]\n",
    "def predicted(model,input_string):\n",
    "   \n",
    "  llm=llm = ChatOllama(model=model, temperature=0, format='json')\n",
    "  message=[\n",
    "  (\"system\",system_prompt),(\"human\",f\"context:{input_string}\")\n",
    "]\n",
    "  ai_msg=llm.invoke(message)\n",
    "  print(\"AI Message Content:\", ai_msg.content.strip())\n",
    "  return ai_msg.content.strip()\n",
    "\n",
    "def mask_term(context, term):\n",
    "    pattern = rf'\\b{re.escape(term)}\\b'  # Match only the whole word\n",
    "    masked_context = re.sub(pattern, \"<fill_here>\", context)\n",
    "    return masked_context\n",
    "\n",
    "def get_random_options(terms_list, correct_term):\n",
    "    if len(terms_list) < 4:\n",
    "        raise ValueError(\"The list must contain at least four terms.\")\n",
    "    options = random.sample(terms_list, 4)\n",
    "    if correct_term not in options:\n",
    "        options[random.randint(0, 3)] = correct_term\n",
    "    return options\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"/home/sunny/Desktop/SunnyKG/prev_method/data_25_37_36.csv\")\n",
    "all_abb = dict(zip(data['name'], data['full_form']))\n",
    "vocab = list(data['name'].unique())\n",
    "masked_data = {}\n",
    "accuracy={}\n",
    "for model in models:\n",
    "  corrected=0\n",
    "  total=len(vocab)\n",
    "  for index, row in data.iterrows():\n",
    "      name = row['name']\n",
    "      content = row['context']\n",
    "      masked_context = mask_term(content, name)\n",
    "      options = get_random_options(vocab, name)\n",
    "      input_string=str({\"masked_context\":masked_context,\"options\":options\n",
    "          \n",
    "      })\n",
    "      \n",
    "      corrected+=(json.loads(predicted(model,input_string))['Selected word'].lower())==name.lower()\n",
    "  print(f\"model {model} given this much accuracy for context\")\n",
    "  accuracy[model]=corrected/total\n",
    "  \n",
    "  \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
